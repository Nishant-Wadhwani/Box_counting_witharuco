# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
import cv2
import imutils
import numpy as np 
import torch
import glob
from torch.utils.tensorboard import SummaryWriter

# default `log_dir` is "runs" - we'll be more specific here
writer = SummaryWriter('runs/fashion_mnist_experiment_1')
from torchvision import transforms as T
import torch.nn.functional as F
from PIL import Image
from maskrcnn_benchmark.modeling.detector import build_detection_model
from maskrcnn_benchmark.utils.checkpoint import DetectronCheckpointer
from maskrcnn_benchmark.structures.image_list import to_image_list
from maskrcnn_benchmark.modeling.roi_heads.mask_head.inference import Masker
from maskrcnn_benchmark import layers as L
from maskrcnn_benchmark.utils import cv2_util
import torchvision.transforms as torch_transform
import numpy as np
import matplotlib
import matplotlib.pyplot as plt


class inference_engine(object):
    # COCO categories for pretty print
    CATEGORIES =["__background__", "Carton Box","Aruco Marker",]
    #CATEGORIES = [
    #    "__background__",
    #    "plane", "ship", "storage-tank", "baseball-diamond", "tennis-court", "basketball-court", "ground-track-field", "harbor", "bridge", "large-vehicle", "small-vehicle", "helicopter", "roundabout", "soccer-ball-field", "swimming-pool", "container-crane",]

    def __init__(
        self,
        cfg,
        weights,
        #confidence_threshold=0.5,
        confidence_threshold=0.75,
        min_image_size=864,
    ):
        self.cfg = cfg.clone()
        self.model = build_detection_model(cfg)
        self.model.eval()
        self.device = torch.device(cfg.MODEL.DEVICE)
        self.model.to(self.device)
        self.min_image_size = min_image_size

        save_dir = cfg.OUTPUT_DIR
        checkpointer = DetectronCheckpointer(cfg, self.model, save_dir=save_dir)
        _ = checkpointer.load(weights)

        self.transforms = self.build_transform()

        # used to make colors for each class
        self.palette = torch.tensor([2 ** 25 - 1, 2 ** 15 - 1, 2 ** 21 - 1])

        self.cpu_device = torch.device("cpu")
        self.confidence_threshold = confidence_threshold


    def build_transform(self):
        """
        Creates a basic transformation that was used to train the models
        """
        cfg = self.cfg

        # we are loading images with OpenCV, so we don't need to convert them
        # to BGR, they are already! So all we need to do is to normalize
        # by 255 if we want to convert to BGR255 format, or flip the channels
        # if we want it to be in RGB in [0-1] range.
        if cfg.INPUT.TO_BGR255:
            to_bgr_transform = T.Lambda(lambda x: x * 255)
        else:
            to_bgr_transform = T.Lambda(lambda x: x[[2, 1, 0]])

        normalize_transform = T.Normalize(
            mean=cfg.INPUT.PIXEL_MEAN, std=cfg.INPUT.PIXEL_STD
        )

        transform = T.Compose(
            [
                T.ToPILImage(),
                T.Resize(self.min_image_size),
                T.ToTensor(),
                to_bgr_transform,
                normalize_transform,
            ]
        )
        return transform

    def run_on_opencv_image(self, image):
        """
        Arguments:
            image (np.ndarray): an image as returned by OpenCV

        Returns:
            prediction (BoxList): the detected objects. Additional information
                of the detection properties can be found in the fields of
                the BoxList via `prediction.fields()`
        """
        predictions = self.compute_prediction(image)
        top_predictions = self.select_top_predictions(predictions)

        #print ("top_predictions = ", top_predictions.quad_bbox)
        #print ("top_predictions = ", top_predictions.get_field("labels"))

        result = image.copy()
        result = self.overlay_boxes(result, top_predictions)
        result = self.overlay_class_names(result, top_predictions)
        dim=(result.shape[1],result.shape[0])
        return result

    def compute_prediction(self, original_image):
        """
        Arguments:
            original_image (np.ndarray): an image as returned by OpenCV

        Returns:
            prediction (BoxList): the detected objects. Additional information
                of the detection properties can be found in the fields of
                the BoxList via `prediction.fields()`
        """
        # apply pre-processing to image
        image = self.transforms(original_image)
        image = image.to(self.device)
        # convert to an ImageList, padded so that it is divisible by
        # cfg.DATALOADER.SIZE_DIVISIBILITY
        image_list = to_image_list(image, self.cfg.DATALOADER.SIZE_DIVISIBILITY)
        image_list = image_list.to(self.device)
        # writer.add_graph(self.model, image_list.tensors)
        # compute predictions
        with torch.no_grad():
            predictions,sal_map,feature = self.model(image_list.tensors, writer)
               

        predictions = [o.to(self.cpu_device) for o in predictions]

        # always single image is passed at a time
        prediction = predictions[0]

        # reshape prediction (a BoxList) into the original image size
        height, width = original_image.shape[:-1]
        prediction = prediction.resize((width, height))
        # print(prediction.quad_bbox)

        # return prediction,smap,sal_map, feature
        return prediction

    def select_top_predictions(self, predictions):
        """
        Select only predictions which have a `score` > self.confidence_threshold,
        and returns the predictions in descending order of score

        Arguments:
            predictions (BoxList): the result of the computation by the model.
                It should contain the field `scores`.

        Returns:
            prediction (BoxList): the detected objects. Additional information
                of the detection properties can be found in the fields of
                the BoxList via `prediction.fields()`
        """
        scores = predictions.get_field("scores")
        keep = torch.nonzero(scores > self.confidence_threshold).squeeze(1)
        predictions = predictions[keep]
        scores = predictions.get_field("scores")
        # print(scores)
        _, idx = scores.sort(0, descending=True)
        return predictions[idx]

    def compute_colors_for_labels(self, labels):
        """
        Simple function that adds fixed colors depending on the class
        """
        colors = labels[:, None] * self.palette
        colors = (colors % 255).numpy().astype("uint8")
        return colors

    def overlay_boxes(self, image, predictions):
        """
        Adds the predicted boxes on top of the image

        Arguments:
            image (np.ndarray): an image as returned by OpenCV
            predictions (BoxList): the result of the computation by the model.
                It should contain the field `labels`.
        """
        labels = predictions.get_field("labels")
        boxes = predictions.bbox
        # print(boxes)
        quad_boxes = predictions.quad_bbox
        print(quad_boxes.shape)
        # print(image.shape)

        colors = self.compute_colors_for_labels(labels).tolist()
        # colors=(128,128,255)

        for quad_box, box, color in zip(quad_boxes, boxes, colors):
            # box = box.to(torch.int64)
            quad_box = quad_box.to(torch.int64)
            # print(quad_box)
            # print(box)
            # top_left, bottom_right = box[:2].tolist(), box[2:].tolist()
            # image = cv2.rectangle(
            #     image, tuple(top_left), tuple(bottom_right), tuple((0,0,255)), 3
            # )
            """cv2.line(image, (quad_box[0], quad_box[1]), (quad_box[2], quad_box[3]), color, 2)
            cv2.line(image, (quad_box[2], quad_box[3]), (quad_box[4], quad_box[5]), color, 2)
            cv2.line(image, (quad_box[4], quad_box[5]), (quad_box[6], quad_box[7]), color, 2)
            cv2.line(image, (quad_box[6], quad_box[7]), (quad_box[0], quad_box[1]), color, 2)"""
            cv2.line(image, (quad_box[0], quad_box[1]), (quad_box[2], quad_box[3]), color, 15)
            cv2.line(image, (quad_box[2], quad_box[3]), (quad_box[4], quad_box[5]), color, 15)
            cv2.line(image, (quad_box[4], quad_box[5]), (quad_box[6], quad_box[7]), color, 15)
            cv2.line(image, (quad_box[6], quad_box[7]), (quad_box[0], quad_box[1]), color, 15)


        return image

    def overlay_class_names(self, image, predictions):
        """
        Adds detected class names and scores in the positions defined by the
        top-left corner of the predicted bounding box

        Arguments:
            image (np.ndarray): an image as returned by OpenCV
            predictions (BoxList): the result of the computation by the model.
                It should contain the field `scores` and `labels`.
        """
        scores = predictions.get_field("scores").tolist()
        # print(scores)
        labels = predictions.get_field("labels").tolist()
        # print(labels)
        labels = [self.CATEGORIES[i] for i in labels]
        boxes = predictions.bbox

        template = "{}: {:.2f}"
        for box, score, label in zip(boxes, scores, labels):
            x, y = box[:2]
            s = template.format(label, score)
            cv2.putText(
                image, s, (x, y), cv2.FONT_HERSHEY_SIMPLEX, .5, (255, 255, 255), 1
            )

        return image

import os
import time
from maskrcnn_benchmark.config import cfg
import argparse
if __name__ == '__main__':
    ################################## Input arguments ##############################################
    

    parser = argparse.ArgumentParser(description=" Required inputs ")
    #parser.add_argument("--img_dir", default = "../datasets/WCCL/img", help="Directory path to image files.", type=str)
    parser.add_argument("--img_dir", default = "/home/gyanesh/Projects/WCCL/Database/Set_01042020/C9N", help="Directory path to image files.", type=str)
    #parser.add_argument("--img_dir", default = "/home/gyanesh/Projects/WCCL/Database/Set_test/CN", help="Directory path to image files.", type=str)
    parser.add_argument("--video_dir", default = "../datasets/WCCL/", help="Directory path to image files.", type=str)
    parser.add_argument("--cfg", default = "../configs/e2e_r2cnn_R_50_FPN_1x_webdemo.yaml", help="Directory path to cfg file.", type=str)
    parser.add_argument("--weight", default = "../output/WCCL3/model_0240000.pth", help="Directory path to cfg file.", type=str)
    parser.add_argument("--out_dir", default = "../datasets/WCCL/demo", help="Directory path to output file.", type=str)
    args = parser.parse_args()
    #print("image dir ---->",args.img_dir)
    #print("video dir ---->",args.video_dir)
    #print("cfg dir ---->",args.cfg)
    #print("weight dir ---->",args.weight)
    #print("output dir ---->",args.out_dir)

    ###########################################################################################
    test_img_dir = args.img_dir
    test_video_dir = args.video_dir
    out_dir = args.out_dir
    config_file = args.cfg
    weights = args.weight
    img_lists = os.listdir(test_img_dir)
    cfg.merge_from_file(config_file)
    detector = inference_engine(cfg, weights)
    #fnames = glob.glob(test_img_dir + "/*.jpg")
    fnames = glob.glob(test_img_dir + "/*.PNG")
    #cap = cv2.VideoCapture(test_video_dir +'Test2.mp4') # for video file
    cap = cv2.VideoCapture(0) # for web cam
    ##################################### This section for Webcam and video ################################################
    '''comment this section if you want to 
       predict images from folder
    '''

    """c=1
    if (cap.isOpened()== False):
        print("Error opening video  file")
    while(cap.isOpened()):
        ret, frame = cap.read()
        if ret == True:
            start_time = time.time()
            canvas = detector.run_on_opencv_image(frame)
            end = time.time()
            print("time taken for detection:", end-start_time)
            out_path = os.path.join(out_dir, "%d.jpg"%c)
            cv2.imwrite(out_path, canvas)
            writer.close()
            #print(frame.shape)
            #cv2.imshow('Frame', frame)
            #if cv2.waitKey(25) & 0xFF == ord('q'):
            #  break
        c=c+1
    cap.release()
    #cv2.destroyAllWindows()"""
    ##############################################################################################################
    #######################################This section for images from folder #######################################
    '''comment this section if you want to 
       predict on webcam or video
    '''
    for f in fnames:
        # print(f)
        out_name = f.split('/')[-1]
        # print("out name ------->",out_name)
        start_time = time.time()
        img = cv2.imread(f)
        # print(img.shape)
        canvas = detector.run_on_opencv_image(img)
        end = time.time()
        # print(img.shape)
        print("time taken for detection:", end-start_time)
        out_path = os.path.join(out_dir, out_name)
        canvas = imutils.resize(canvas, width=1000)
        cv2.imwrite(out_path, canvas)
        writer.close()
        #cv2.imshow("COCO detections", canvas)
        #if cv2.waitKey(1) == 27:
        #    break  # esc to quit
        #count+=1
    cv2.destroyAllWindows()
